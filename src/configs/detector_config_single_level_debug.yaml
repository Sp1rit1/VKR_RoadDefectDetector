# RoadDefectDetector/src/configs/detector_config_single_level_debug.yaml

# --- Параметры для ОДНОУРОВНЕВОЙ (P4) модели детектора для ОТЛАДКИ ---
# Цель: Идеально переобучиться на ОДНОМ изображении с ДВУМЯ объектами P4.

fpn_detector_params: # Название секции оставляем для совместимости
  model_name_prefix: "RoadDefectDetector_Debug_P4_TwoObjects"
  backbone_name: "MobileNetV2"
  input_shape: [416, 416, 3] # Оставляем стандартный, как в основном конфиге
  classes: ["pit", "crack"]  # Убедись, что объекты на твоем изображении этих классов
  num_classes: 2

  # --- Параметры для ОДНОГО УРОВНЯ (P4_debug) ---
  detector_fpn_levels: ['P4_debug'] # Наш единственный уровень
  detector_fpn_strides:
    P4_debug: 16 # Страйд для выхода MobileNetV2 'block_13_expand_relu'

  fpn_gt_assignment_scale_ranges: # Один широкий диапазон, чтобы все объекты точно попали сюда
    P4_debug: [0, 100000]       # (Для отладки на одном изображении это не так важно,
                                #  важнее, чтобы якоря подходили)

  detector_fpn_anchor_configs:
    P4_debug:
      num_anchors_this_level: 7 # <<< МОЖНО ПОПРОБОВАТЬ 3-5 ЯКОРЕЙ ДЛЯ P4,
                                # взятых из твоего основного конфига (K-Means для P4)
                                # Если у тебя два объекта РАЗНЫХ форм/размеров,
                                # несколько якорей дадут больше шансов на хорошее сопоставление.
                                # Если ОДИН якорь [0.15, 0.15] плохо покрывает ОБА твоих объекта,
                                # модель может испытывать трудности.
                                # Давай используем ТРИ якоря, которые могут подойти для средних объектов P4:
      anchors_wh_normalized: # Примерные средние якоря, ЗАМЕНИ на твои K-Means для P4, если они лучше
        - [0.1832, 0.1104]
        - [0.0967, 0.2753]
        - [0.4083, 0.0925]
        - [0.0921, 0.4968]
        - [0.2919, 0.1936]
        - [0.7358, 0.0843]
        - [0.0743, 0.8969]

  head_config:
    fpn_filters: 256 # Не используется
    head_depth: 2
    head_conv_filters: 128 # Можно оставить 128 или вернуть 256 для большей мощности головы
    leaky_relu_alpha: 0.1
    l2_regularization: null # Отключаем регуляризацию для максимального переобучения

# --- Параметры Обучения для Отладки ---
unfreeze_backbone: false # Backbone ЗАМОРОЖЕН для этого теста (обучаем только голову)

batch_size: 1
epochs_for_debug: 500    # <<< УВЕЛИЧИМ количество итераций, чтобы точно переобучиться
initial_learning_rate: 0.001 # <<< МОЖНО ПОПРОБОВАТЬ LR ЧУТЬ ВЫШЕ (0.001 или 0.0005) для ускорения переобучения на одном примере,
                           # но если loss будет скакать, вернем 0.0001

# use_augmentation: false - подразумевается, что в debug_single_level_train_on_one_image.py аугментация отключена

# --- Параметры Функции Потерь для Отладки ---
loss_weights:
  coordinates: 1.5      # <<< УВЕЛИЧИМ вес для координат, чтобы рамки были точнее
  objectness: 5.0       # <<< ВЫСОКИЙ вес, чтобы модель уверенно находила объекты
  no_object: 0.1        # <<< ОЧЕНЬ НИЗКИЙ вес для фона, чтобы не мешать находить объекты
  classification: 2.0   # <<< УВЕЛИЧИМ вес для правильной классификации

focal_loss_objectness_params:
  use_focal_loss: false # Отключаем Focal Loss для простоты отладки базового BCE
  alpha: 0.25
  gamma: 2.0

# --- Параметры для Инференса (для визуализации в отладке) ---
predict_params:
  confidence_threshold: 0.3 # <<< НИЗКИЙ порог, чтобы видеть даже слабые попытки модели
  iou_threshold: 0.3
  max_detections: 10