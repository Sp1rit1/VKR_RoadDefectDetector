## Проект: Обнаружение ям с помощью квадрокоптера с камерой

## Команды участвовавшие в разработке проекта
- Команда Грок - Костюнин Роман Евгеньевич(АУБП-23), Фролова Лада Дмитриевна(АУБП-23), Сотников Алексей Александрович(АУБП-23), Кузнецов Данил Константинович(АУБП-23), Выдрин Владимир Максимович(НФ-23).
- Команда OMG - Храмцов Дмитрий Олегович(АУБП-23), Симонова Арина Игоревна(АУБП-23), Тайболин Яков Геннадьевич(АУБП-23), Порошин Артем Русланович(АУБП-23),  Панин Владислав Сергеевич(АУБП-23).
- Команда Шахи - Неуймин Нестор Константинович(ШС-23),  Бачурин Николай Алексеевич(ШС-23), Бехтерев Илья Алексеевич(ШС-23), Игнатьев Константин Николаевич(ШС-23)

## Цель работы  
- Разработка и реализация прототипа системы, предназначенной для мониторинга состояния дорожного покрытия. Система использует беспилотный летательный аппарат (БПЛА), оснащённый камерой, для съёмки дорожного полотна, передаёт полученные изображения на сервер, где в дальнейшем осуществляется их обработка с использованием методов компьютерного зрения и искусственного интеллекта.

## Ожидаемый результат  
- Система аэрофотосъемки дорожного покрытия
- Алгоритм компьютерного зрения для автоматического обнаружения дефектов
- Карта ям с точными координатами для планирования ремонтных работ

## Стек технлогий 
- Языки программирования: Python
- Фреймворки: ROS (Robot Operating System)
- Библиотеки компьютерного зрения: OpenCV 
- Сетевое взаимодействие: HTTP через библиотеку requests
- Симулятор/платформа: Clover COEX 4 (квадрокоптерная платформа)

## Аппаратная часть 
- Проект реализован для квадрокоптера Clover COEX 4 , включая:
  - Основной квадрокоптер с полетным контроллером
  - Камеру для съемки дорожного покрытия
  - Светодиодную индикацию для визуального отображения состояния системы

## Программная реализация   
- Квадрокоптер выполняет запрограммированный маршрут съемки 
- На каждой точке маршрута делается снимок дорожного покрытия
- Изображение передается на сервер для анализа
- Сервер детектирует состояние дороги:
  - Не дорога
  - Нормальная дорога
  - Трещина
  - Яма
- В зависимости от ответа сервера:
  - При обнаружении серьёзных дефектов, таких как «яма» или «трещина», дрон использует цветовую идентификацию
  - При состоянии "не дорога" возвращается на исходную позицию
  - При состоянии "нормальная дорога" продолжает маршрут
- Все данные сохраняются для последующего анализа и составления карты дефектов

## Нейросетевая Подсистема Детекции Дефектов
Для автоматического обнаружения дефектов дорожного полотна была разработана и обучена специализированная нейросетевая модель.
Архитектура Модели (DetectorV3_RetinaNet)
В качестве основы для детектора объектов была выбрана архитектура, сочетающая в себе современные и проверенные подходы, аналогичные RetinaNet, для достижения оптимального баланса между точностью и производительностью:
-Backbone (Опорная сеть): Используется EfficientNetB0, предварительно обученная на датасете ImageNet. Эта сеть эффективно извлекает иерархические признаки из входного изображения (512x512x3 пикселей).
-Neck (Шея): Реализована Feature Pyramid Network (FPN), которая агрегирует карты признаков с разных уровней Backbone (P3, P4, P5, соответствующие страйдам 8, 16, 32). Это позволяет модели эффективно обнаруживать объекты различных масштабов.
-Heads (Головы предсказаний): К каждому из трех уровней FPN (P3, P4, P5) применяются общие (shared weights) сверточные головы. Каждая голова имеет две ветки:
  -Ветка классификации: Предсказывает класс объекта ( "pit" - яма, "crack" - трещина) для каждого якоря.
  -Ветка регрессии: Предсказывает смещения для уточнения координат ограничивающей рамки (bounding box) для каждого якоря.
-Якоря (Anchors): Используется стандартный для RetinaNet подход генерации якорей на основе 3-х масштабов (anchor_scales) и 5-ти соотношений сторон (anchor_ratios), что дает 15 якорей на каждую ячейку сетки признаков. Соотношения сторон были подобраны на основе анализа геометрии объектов в обучающем датасете.
-Кастомная реализация: Модель реализована как наследник tf.keras.Model с переопределенными методами train_step и test_step для полного контроля над процессом обучения и валидации.

## Подготовка Данных, Обучение и Оценка
-Датасет: Для обучения использовался собственный датасет из ~4300 изображений дорожного полотна с аннотациями дефектов в формате PASCAL VOC. Целевые классы: "pit" (яма) и "crack" (трещина). Датасет разделен на обучающую (~3200 изображений) и валидационную (~1079 изображений) выборки.
-Анализ Данных и Якоря: Проведен анализ распределения размеров и соотношений сторон объектов. Финальным решением стал переход к стандартному для RetinaNet подходу с 5 соотношениями сторон ([0.5, 0.8, 1.0, 1.3, 2.0]) и 3 масштабами, что дает 15 якорей на ячейку.
-Загрузчик Данных (DataGenerator):
  -Реализован кастомный генератор данных.
  Предобработка: Ресайз до 512x512, нормализация (аналогично EfficientNetB0.preprocess_input).
  -Аугментация (для обучения): Применялся набор аугментаций из Albumentations (отражения, аффинные преобразования, изменения яркости/контраста/HSV, размытие) со смягченными порогами отсечки.
  -Формирование Ground Truth (y_true):
  GT-объекты назначаются на уровни FPN (P3, P4, P5) по максимальной стороне рамки.
  Якорям на соответствующем уровне FPN присваиваются метки (позитивный, негативный, игнорируемый) на основе IoU (пороги 0.4 для позитивных, 0.3 для игнорируемых). Реализовано "гарантированное назначение".
  Для позитивных якорей кодируются регрессионные цели (tx, ty, tw, th) с масштабированием и клиппингом.
  y_true для классификации формируется как one-hot вектор (с меткой -1 для игнорируемых).
-Функция Потерь (DetectorLoss):
  -Классификация: Focal Loss (alpha=0.25, gamma=1.0).
  -Регрессия: CIoU Loss (Complete IoU Loss).
  -Обе компоненты потерь нормализуются на количество позитивных якорей. Общий вес потерь регрессии = 1.3.
-Процесс Обучения:
  -Двухфазное обучение: Фаза 1 (50 эпох, замороженный Backbone и все BN-слои, LR ~1.6e-3), Фаза 2 (30 эпох, fine-tuning Backbone и BN-слоев, LR ~1.6e-4).
  -Оптимизатор: AdamW с clipnorm=1.0. Планировщик LR: WarmupCosineDecay.
  -Использовался EarlyStopping (patience=15) с восстановлением лучших весов.
  
-Результаты Обучения (CPU):
Метрики на валидации (IoU порог = 0.1):
Класс "pit": Precision = 0.8785, Recall = 0.6300, F1-score = 0.7338
Класс "crack": Precision = 0.7967, Recall = 0.6242, F1-score = 0.7000
Общий Micro-F1-score: 0.7146
Высокий Precision указывает на надежность детекций
Сравнение с YOLOv8n (IoU порог = 0.1):
YOLOv8n: Micro-F1-score = 0.7787.
Разработанная модель показала сопоставимый Precision и достигла значимого F1-score.



## Основная логика полета и обработки изображений:

import time
import requests
import rospy
import cv2
from cv_bridge import CvBridge
from sensor_msgs.msg import Image
from clover import srv
from std_srvs.srv import Trigger
from clover.srv import SetLEDEffect
import math

rospy.init_node('flight')

get_telemetry = rospy.ServiceProxy('get_telemetry', srv.GetTelemetry)
navigate = rospy.ServiceProxy('navigate', srv.Navigate)
set_effect = rospy.ServiceProxy('led/set_effect', SetLEDEffect)
land = rospy.ServiceProxy('land', Trigger)

def navigate_wait(x=0, y=0, z=0, yaw=float('nan'), speed=0.5, frame_id='', auto_arm=False, tolerance=0.2):
    print(f"Лечу в точку: x={x}, y={y}, z={z}, frame_id={frame_id}")
    navigate(x=x, y=y, z=z, yaw=yaw, speed=speed, frame_id=frame_id, auto_arm=auto_arm)
    while not rospy.is_shutdown():
        telem = get_telemetry(frame_id='navigate_target')
        distance = math.sqrt(telem.x ** 2 + telem.y ** 2 + telem.z ** 2)
        if distance < tolerance:
            print("Точка достигнута!")
            break
        rospy.sleep(0.2)

 ... (полный код в файле проекта)

## Перспективы развития  
- Интеграция модуля машинного зрения для анализа изображений в реальном времени  
- Добавление геолокации и привязки данных к координатам  
- Использование более мощных моделей нейросетей для точного распознавания дефектов  
- Разработка интерфейса отображения результатов анализа
