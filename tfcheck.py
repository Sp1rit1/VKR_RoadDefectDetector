–†–æ–º–∞, [24.06.2025 17: 22]
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/augmentations.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import albumentations as A
from packaging import version


def get_detector_train_augmentations(img_size: int) -> A.Compose:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞–π–ø-–ª–∞–π–Ω –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å Albumentations < 1.3 (–ø–∞—Ä–∞–º–µ—Ç—Ä border_mode),
    —Ç–∞–∫ –∏ —Å –±–æ–ª–µ–µ –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä mode).
    """
    ver = version.parse(A.version)
    kw_border = {"border_mode" if ver < version.parse("1.3.0") else "mode": A.BORDER_REFLECT_101}

    return A.Compose(
        [
            A.HorizontalFlip(p=0.5),
            A.ShiftScaleRotate(
                shift_limit=0.05,
                scale_limit=0.15,
                rotate_limit=10,
                **kw_border,
                p=0.7,
            ),
            A.RandomBrightnessContrast(0.15, 0.15, p=0.6),
            A.HueSaturationValue(10, 10, 10, p=0.4),
            A.CLAHE(p=0.1),
            A.Resize(img_size, img_size),
        ],
        bbox_params=A.BboxParams(format="pascal_voc", label_fields=["category_ids"]),
    )


–†–æ–º–∞, [24.06.2025 17: 22]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/data_loader_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def create_dataset(generator, batch_size: int, is_training: bool) -> tf.data.Dataset:
    """
    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç DataGenerator –≤ tf.data.Dataset.
    ‚Ä¢ shuffle –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –±—É—Ñ–µ—Ä–µ ‚â•1 –æ–±—Ä–∞–∑–µ—Ü
    ‚Ä¢ drop_remainder –≤—ã–∫–ª—é—á–µ–Ω, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    """
    dataset = tf.data.Dataset.from_generator(
        lambda: generator,
        output_signature=generator.output_signature,
    )

    # –ë—É—Ñ–µ—Ä = 3√óbatch –∏–ª–∏ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç, —á—Ç–æ –º–µ–Ω—å—à–µ
    buffer_size = max(1, min(len(generator), batch_size * 3))
    if is_training:
        dataset = dataset.shuffle(buffer_size)

    dataset = dataset.batch(batch_size, drop_remainder=False)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset


–†–æ–º–∞, [24.06.2025 17: 22]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/generator.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class DataGenerator:
    def init(self, cfg, annotations, *args, **kwargs):
        ...
        # anchors
        self.anchor_scales = cfg['anchor_scales']
        self.anchor_ratios = cfg['anchor_ratios']
        self.all_anchors = generate_anchors(self.anchor_scales, self.anchor_ratios, ...)

        # <<<   –ë–´–õ assert, –∫–æ—Ç–æ—Ä—ã–π –≤–∞–ª–∏–ª –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä   >>>
        expected_per_level = len(self.anchor_scales) * len(self.anchor_ratios)
        if cfg['num_anchors_per_level'] != expected_per_level:
            logging.warning(
                "num_anchors_per_level=%d –≤ –∫–æ–Ω—Ñ–∏–≥–µ, –Ω–æ –ø–æ —Ä–∞—Å—á—ë—Ç—É –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å %d. "
                "–ò—Å–ø–æ–ª—å–∑—É—é –≤—ã—á–∏—Å–ª—ë–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.",
                cfg['num_anchors_per_level'], expected_per_level,
            )
            cfg['num_anchors_per_level'] = expected_per_level
        self.total_anchors_count = expected_per_level * sum(
            fmap_h * fmap_w for fmap_h, fmap_w in self.feature_map_shapes
        )


–†–æ–º–∞, [24.06.2025 17: 22]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/models/build_detector_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_detector_v3_standard(cfg):
    ...
    # heads
    reg_p3 = build_regression_head(P3, num_anchors)
    cls_p3 = build_classification_head(P3, num_anchors, num_classes)
    reg_p4 = build_regression_head(P4, num_anchors)
    cls_p4 = build_classification_head(P4, num_anchors, num_classes)
    reg_p5 = build_regression_head(P5, num_anchors)
    cls_p5 = build_classification_head(P5, num_anchors, num_classes)

    # üÜï  –µ–¥–∏–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ ¬´—Ä–µ–≥-–∫–ª–∞—Å—Å¬ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    outputs = [reg_p3, cls_p3, reg_p4, cls_p4, reg_p5, cls_p5]

    return keras.Model(inputs=inputs, outputs=outputs, name="road_defect_detector")


–†–æ–º–∞, [24.06.2025 17: 23]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/losses/detection_losses_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class DetectorLoss(keras.losses.Loss):
    ...

    def call(self, y_true, y_pred):
        """
        –û–∂–∏–¥–∞–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –≤—Ö–æ–¥–æ–≤:
        [reg_P3, cls_P3, reg_P4, cls_P4, ‚Ä¶]
        """
        # ------------------------------------
        # 1. —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –Ω–µ—á—ë—Ç/—á—ë—Ç (0::2 / 1::2)
        reg_tensors = y_pred[0::2]
        cls_tensors = y_pred[1::2]

        # 2. –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
        reg_flat = tf.concat(
            [tf.reshape(t, [tf.shape(t)[0], -1, 4]) for t in reg_tensors],
            axis=1,
        )
        cls_flat = tf.concat(
            [tf.reshape(t, [tf.shape(t)[0], -1, self.num_classes]) for t in cls_tensors],
            axis=1,
        )

        y_true_reg_flat, y_true_cls_flat = y_true  # —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –º–µ–Ω—è–ª–æ—Å—å

        # 3. sanity-check
        tf.debugging.assert_equal(
            tf.shape(y_true_reg_flat),
            tf.shape(reg_flat),
            message="DetectorLoss: y_true/y_pred reg shapes differ",
        )

        # 4. —Å—á–∏—Ç–∞–µ–º –ª–æ—Å—Å—ã ‚Ä¶
        reg_loss = self.box_loss_fn(y_true_reg_flat, reg_flat)
        cls_loss = self.cls_loss_fn(y_true_cls_flat, cls_flat)
        total = self.box_loss_weight * reg_loss + cls_loss

        return total


–†–æ–º–∞, [24.06.2025 17: 23]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/callbacks/warmup_cosine_decay.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class WarmupCosineDecay(keras.callbacks.Callback):
    def init(self, base_lr, warmup_steps, total_steps, **kwargs):
        super().init(**kwargs)
        self.base_lr = base_lr
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self._start_iter = None  # ‚Üê –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω –Ω–∞ on_train_begin

    def on_train_begin(self, logs=None):
        # —Ñ–∏–∫—Å–∏—Ä—É–µ–º ¬´–Ω–æ–ª—å¬ª –≤ –º–æ–º–µ–Ω—Ç —Å—Ç–∞—Ä—Ç–∞ —Ñ–∞–∑—ã, –∞ –Ω–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–±—ç–∫–∞
        self._start_iter = int(self.model.optimizer.iterations)

    def on_train_batch_begin(self, batch, logs=None):
        current = int(self.model.optimizer.iterations) - self._start_iter
        ...


–†–æ–º–∞, [24.06.2025 17: 23]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tests/test_detector_loss.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def test_detector_loss_shapes():
    ...
    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –Ω–µ—è–≤–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π y_pred
    y_pred = [reg_P3, cls_P3, reg_P4, cls_P4, reg_P5, cls_P5]

    loss_fn = DetectorLoss(num_classes=2, ...)
    loss = loss_fn(y_true, y_pred)
    assert loss > 0.0


–†–æ–º–∞, [24.06.2025 17: 23]
–ù–∏–∂–µ ‚Äí —Ç–æ–ª—å–∫–æ
–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ
—Ñ—É–Ω–∫—Ü–∏–∏(–æ—Å—Ç–∞–ª—å–Ω–æ–π
–∫–æ–¥
—Ç—Ä–æ–≥–∞—Ç—å
–Ω–µ
–Ω—É–∂–Ω–æ).
–ü—É—Ç—å
–∫
—Ñ–∞–π–ª—É
—É–∫–∞–∑–∞–Ω
–≤
–ø–æ–¥–ø–∏—Å–∏
–∫–∞–∂–¥–æ–≥–æ
–±–ª–æ–∫–∞, —á—Ç–æ–±—ã
–±—ã–ª–æ
–ø–æ–Ω—è—Ç–Ω–æ, –∫—É–¥–∞
–≤—Å—Ç–∞–≤–ª—è—Ç—å.

---

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/augmentations.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import albumentations as A
from packaging import version


def get_detector_train_augmentations(img_size: int) -> A.Compose:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞–π–ø-–ª–∞–π–Ω –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞.
    –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å Albumentations < 1.3 (–ø–∞—Ä–∞–º–µ—Ç—Ä border_mode),
    —Ç–∞–∫ –∏ —Å –±–æ–ª–µ–µ –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ (–ø–∞—Ä–∞–º–µ—Ç—Ä mode).
    """
    ver = version.parse(A.version)
    kw_border = {"border_mode" if ver < version.parse("1.3.0") else "mode": A.BORDER_REFLECT_101}

    return A.Compose(
        [
            A.HorizontalFlip(p=0.5),
            A.ShiftScaleRotate(
                shift_limit=0.05,
                scale_limit=0.15,
                rotate_limit=10,
                **kw_border,
                p=0.7,
            ),
            A.RandomBrightnessContrast(0.15, 0.15, p=0.6),
            A.HueSaturationValue(10, 10, 10, p=0.4),
            A.CLAHE(p=0.1),
            A.Resize(img_size, img_size),
        ],
        bbox_params=A.BboxParams(format="pascal_voc", label_fields=["category_ids"]),
    )


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/data_loader_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def create_dataset(generator, batch_size: int, is_training: bool) -> tf.data.Dataset:
    """
    –û–±–æ—Ä–∞—á–∏–≤–∞–µ—Ç DataGenerator –≤ tf.data.Dataset.
    ‚Ä¢ shuffle –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –±—É—Ñ–µ—Ä–µ ‚â•1 –æ–±—Ä–∞–∑–µ—Ü
    ‚Ä¢ drop_remainder –≤—ã–∫–ª—é—á–µ–Ω, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
    """
    dataset = tf.data.Dataset.from_generator(
        lambda: generator,
        output_signature=generator.output_signature,
    )

    # –ë—É—Ñ–µ—Ä = 3√óbatch –∏–ª–∏ –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç, —á—Ç–æ –º–µ–Ω—å—à–µ
    buffer_size = max(1, min(len(generator), batch_size * 3))
    if is_training:
        dataset = dataset.shuffle(buffer_size)

    dataset = dataset.batch(batch_size, drop_remainder=False)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/datasets/generator.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class DataGenerator:
    def init(self, cfg, annotations, *args, **kwargs):
        ...
        # anchors
        self.anchor_scales = cfg['anchor_scales']
        self.anchor_ratios = cfg['anchor_ratios']
        self.all_anchors = generate_anchors(self.anchor_scales, self.anchor_ratios, ...)

        # <<<   –ë–´–õ assert, –∫–æ—Ç–æ—Ä—ã–π –≤–∞–ª–∏–ª –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä   >>>
        expected_per_level = len(self.anchor_scales) * len(self.anchor_ratios)
        if cfg['num_anchors_per_level'] != expected_per_level:
            logging.warning(
                "num_anchors_per_level=%d –≤ –∫–æ–Ω—Ñ–∏–≥–µ, –Ω–æ –ø–æ —Ä–∞—Å—á—ë—Ç—É –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å %d. "
                "–ò—Å–ø–æ–ª—å–∑—É—é –≤—ã—á–∏—Å–ª—ë–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.",
                cfg['num_anchors_per_level'], expected_per_level,
            )
            cfg['num_anchors_per_level'] = expected_per_level
        self.total_anchors_count = expected_per_level * sum(
            fmap_h * fmap_w for fmap_h, fmap_w in self.feature_map_shapes
        )


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/models/build_detector_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_detector_v3_standard(cfg):
    ...
    # heads
    reg_p3 = build_regression_head(P3, num_anchors)
    cls_p3 = build_classification_head(P3, num_anchors, num_classes)
    reg_p4 = build_regression_head(P4, num_anchors)
    cls_p4 = build_classification_head(P4, num_anchors, num_classes)
    reg_p5 = build_regression_head(P5, num_anchors)
    cls_p5 = build_classification_head(P5, num_anchors, num_classes)

    # üÜï  –µ–¥–∏–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ ¬´—Ä–µ–≥-–∫–ª–∞—Å—Å¬ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    outputs = [reg_p3, cls_p3, reg_p4, cls_p4, reg_p5, cls_p5]

    return keras.Model(inputs=inputs, outputs=outputs, name="road_defect_detector")


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/losses/detection_losses_v3_standard.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class DetectorLoss(keras.losses.Loss):


–†–æ–º–∞, [24.06.2025 17: 23]
...


def call(self, y_true, y_pred):
    """
    –û–∂–∏–¥–∞–µ–º—ã–π –ø–æ—Ä—è–¥–æ–∫ –≤—Ö–æ–¥–æ–≤:
    [reg_P3, cls_P3, reg_P4, cls_P4, ‚Ä¶]
    """
    # ------------------------------------
    # 1. —Ä–∞–∑–¥–µ–ª—è–µ–º –ø–æ –Ω–µ—á—ë—Ç/—á—ë—Ç (0::2 / 1::2)
    reg_tensors = y_pred[0::2]
    cls_tensors = y_pred[1::2]

    # 2. –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º
    reg_flat = tf.concat(
        [tf.reshape(t, [tf.shape(t)[0], -1, 4]) for t in reg_tensors],
        axis=1,
    )
    cls_flat = tf.concat(
        [tf.reshape(t, [tf.shape(t)[0], -1, self.num_classes]) for t in cls_tensors],
        axis=1,
    )

    y_true_reg_flat, y_true_cls_flat = y_true  # —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –º–µ–Ω—è–ª–æ—Å—å

    # 3. sanity-check
    tf.debugging.assert_equal(
        tf.shape(y_true_reg_flat),
        tf.shape(reg_flat),
        message="DetectorLoss: y_true/y_pred reg shapes differ",
    )

    # 4. —Å—á–∏—Ç–∞–µ–º –ª–æ—Å—Å—ã ‚Ä¶
    reg_loss = self.box_loss_fn(y_true_reg_flat, reg_flat)
    cls_loss = self.cls_loss_fn(y_true_cls_flat, cls_flat)
    total = self.box_loss_weight * reg_loss + cls_loss

    return total


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ src/callbacks/warmup_cosine_decay.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class WarmupCosineDecay(keras.callbacks.Callback):
    def init(self, base_lr, warmup_steps, total_steps, **kwargs):
        super().init(**kwargs)
        self.base_lr = base_lr
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
        self._start_iter = None  # ‚Üê –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω –Ω–∞ on_train_begin

    def on_train_begin(self, logs=None):
        # —Ñ–∏–∫—Å–∏—Ä—É–µ–º ¬´–Ω–æ–ª—å¬ª –≤ –º–æ–º–µ–Ω—Ç —Å—Ç–∞—Ä—Ç–∞ —Ñ–∞–∑—ã, –∞ –Ω–µ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–±—ç–∫–∞
        self._start_iter = int(self.model.optimizer.iterations)

    def on_train_batch_begin(self, batch, logs=None):
        current = int(self.model.optimizer.iterations) - self._start_iter
        ...


---


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tests/test_detector_loss.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def test_detector_loss_shapes():
    ...
    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –Ω–µ—è–≤–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π y_pred
    y_pred = [reg_P3, cls_P3, reg_P4, cls_P4, reg_P5, cls_P5]

    loss_fn = DetectorLoss(num_classes=2, ...)
    loss = loss_fn(y_true, y_pred)
    assert loss > 0.0

> –¢–µ—Å—Ç
–æ—Å—Ç–∞–≤–ª–µ–Ω
–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º: –ø—Ä–æ–≤–µ—Ä—è–µ–º
—Ç–æ–ª—å–∫–æ, —á—Ç–æ
—Ñ–æ—Ä–º—É ¬´—Å—ä–µ–¥–∞–µ—Ç¬ª –∏
–ª–æ—Å—Å
—Å—á–∏—Ç–∞–µ—Ç—Å—è.

---

–ö–∞–∫
–ø—Ä–∏–º–µ–Ω—è—Ç—å
–ø–∞—Ç—á–∏

1.
–°–∫–æ–ø–∏—Ä—É–π—Ç–µ
–±–ª–æ–∫
—Å
–Ω—É–∂–Ω—ã–º
–ø—É—Ç—ë–º
–ø–æ–≤–µ—Ä—Ö
—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π
—Ñ—É–Ω–∫—Ü–∏–∏ / –∫–ª–∞—Å—Å–∞
–≤
—Å–≤–æ—ë–º
–ø—Ä–æ–µ–∫—Ç–µ.

2.
–ó–∞–ø—É—Å—Ç–∏—Ç–µ
python - m
pytest ‚Äì —Ç–µ—Å—Ç
–¥–æ–ª–∂–µ–Ω
–ø—Ä–æ–π—Ç–∏.

3.
–ó–∞–ø—É—Å—Ç–∏—Ç–µ
train_detector_v3_standard.py ‚Äì –æ–±—É—á–µ–Ω–∏–µ
—Å—Ç–∞—Ä—Ç—É–µ—Ç
–±–µ–∑
–ø–∞–¥–µ–Ω–∏–π.